<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Python on Bhaskar Karambelkar&#39;s Blog</title>
    <link>/tags/python/index.xml</link>
    <description>Recent content in Python on Bhaskar Karambelkar&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; Bhaskar V. Karambelkar</copyright>
    <atom:link href="/tags/python/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Book Review : Data Driven Security</title>
      <link>/2015/05/book-review--data-driven-security/</link>
      <pubDate>Tue, 05 May 2015 11:50:00 +0000</pubDate>
      
      <guid>/2015/05/book-review--data-driven-security/</guid>
      <description>

&lt;p&gt;&lt;link href=&#34;/css/ddsec.css&#34; rel=&#34;stylesheet&#34;&gt;
&lt;br/&gt;
&lt;div class=&#39;warning-box&#39;&gt;
&lt;strong&gt;&lt;em&gt;&amp;nbsp;Disclosure&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;
I work with the two authors of this book. In fact one of them is my manager. But a) I don&amp;rsquo;t like to suck up to my colleagues and b) I&amp;rsquo;m sure they don&amp;rsquo;t like being sucked up to either. Despite this if you think my review will be biased then stop reading now. Go watch some cat videos.
&lt;/div&gt;
&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://datadrivensecurity.info/book/&#34;&gt;&lt;img src=&#34;http://datadrivensecurity.info/book/theme/images/cover.jpg&#34; alt=&#34;Cover&#34; /&gt;&lt;/a&gt; Data Driven Security is a first of it&amp;rsquo;s kind book that aims to achieve the impossible; To be a book that integrates all 3 dimensions of &amp;lsquo;Data Science&amp;rsquo;, a) Math and Statistical Knowledge, b) Coding/Hacking skills, and c) Domain Knowledge. Domain in this case being the Information Security Domain. If these 3 dimensions are unknown to you, look at the figure on the right..
&lt;a href=&#34;http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram&#34;&gt;&lt;img src=&#34;http://static1.squarespace.com/static/5150aec6e4b0e340ec52710a/t/51525c33e4b0b3e0d10f77ab/1364352052403/Data_Science_VD.png?format=1500w&#34; alt=&#34;DS&#34; /&gt;&lt;/a&gt;
&lt;br/&gt;&lt;br/&gt;
Traditionally books available for data science have tackled only one dimension at a time or at best two. This book is unique in that regard as it tackles all 3 dimensions. This is worth mentioning especially when you consider that concepts like statistical and machine learning are not part of traditional InfoSec tools. Traditional InfoSec tools are based around the concept of signature matching, i.e. determining if a threat matches from a set of already known badness such as a virus, malware, network activity, ip address, domain name. This approach is always playing catch up and the good guys are always one step (in fact several steps) behind the bad guys. This is where data driven security comes in. The idea is to use data analysis techniques for security research and build the next generation of InfoSec tools that can spot badness before it is known. A fascinating field, trust me. The challenge of writing on such a subject can not be overstated. The book needs to be approachable by readers coming in from any of the three dimensions. Also each of the three dimensions is so vast and wide that you can find hundreds of books dedicated to just one single dimension. So have the authors been successful in this endeavor ? Read on to find out&amp;hellip;
&lt;br clear=&#39;both&#39;/&gt;&lt;/p&gt;

&lt;h2 id=&#34;at-a-glance&#34;&gt;At a glance&lt;/h2&gt;

&lt;p&gt;The book (ISBN: 978-1-118-79372-5) is published by &lt;a href=&#34;http://www.wiley.com/WileyCDA/WileyTitle/productCd-1118793722.html&#34;&gt;Wiley Publications&lt;/a&gt; in Feb, 2014. Wiley has been publishing some really interesting titles over the past few years in the Data Analysis, Statistics domain. The book is absolutely gorgeous  from cover to cover. The page quality is very high and a lot of effort has gone into making the code and figures look stunning. It is one of the best visually pleasing books I have in my collection (in addition to anything by &lt;a href=&#34;http://www.amazon.com/Stephen-Few/e/B001H6IQ5M&#34;&gt;Stephen Few&lt;/a&gt; and &lt;a href=&#34;http://www.amazon.com/Edward-R.-Tufte/e/B000APET3Y/&#34;&gt;Edward R. Tufte&lt;/a&gt;). All code presented is properly commented, something I seldom find in technical books. There is a ton of code in this book, which is expected as coding is one of the skills in data science. The authors have done a great job presenting code in &lt;a href=&#34;http://python.org&#34;&gt;python&lt;/a&gt; and &lt;a href=&#34;http://www.r-project.org/&#34;&gt;R&lt;/a&gt;. Both python and R offer libraries and interactive environments for data analysis and by presenting the code in 2 languages the authors have made the book accessible to wide audience.
&lt;br/&gt;In addition to the book the authors have a website: &lt;a href=&#34;http://datadrivensecurity.info/book&#34;&gt;datadrivensecurity.info&lt;/a&gt; for the book, a &lt;a href=&#34;http://datadrivensecurity.info/blog/&#34;&gt;blog&lt;/a&gt;  and a &lt;a href=&#34;http://datadrivensecurity.info/podcast/&#34;&gt;podcast&lt;/a&gt; where they discuss all things InfoSec and data science.&lt;/p&gt;

&lt;h2 id=&#34;chapters&#34;&gt;Chapters&lt;/h2&gt;

&lt;h3 id=&#34;chapter-1-the-journey-to-data-driven-security&#34;&gt;Chapter 1: The Journey to Data Driven Security&lt;/h3&gt;

&lt;p&gt;Chapter 1 starts with a brief history of data analysis, from the classical statistical analysis techniques of the Nineteenth and Twentieth century to the modern algorithmic approaches of the Twenty-First century. You have enough anecdotes to convince you of the importance of data analysis in case you are still wondering why analyze data in the first place. The chapter then explores the skill sets required for data analysis: Domain expertise, Data Management, Programming, Statistics and finally Visualization. Each topic is given its due credit and you&amp;rsquo;ll learn how each of these pieces fits in to the mosaic. The chapter rounds off with a very important section &amp;lsquo;Centering on a Question&amp;rsquo;. It is very much possible that your data analysis can lead you to many directions if you don&amp;rsquo;t have a proper research question framed (Your&amp;rsquo;s truly is guilty of this many times over). In short you learn the history of data analysis, skill-sets required for it and the importance of framing the right question(s).&lt;/p&gt;

&lt;h3 id=&#34;chapter-2-building-your-analytics-toolbox-a-primer-on-using-r-and-python-for-security-analysis&#34;&gt;Chapter 2: Building Your Analytics Toolbox: A Primer on Using R and Python for Security Analysis&lt;/h3&gt;

&lt;p&gt;Chapter 2 is all about setting up &lt;a href=&#34;http://www.python.org/&#34;&gt;Python&lt;/a&gt; and &lt;a href=&#34;http://www.r-project.org/&#34;&gt;R&lt;/a&gt; development environments for your data analysis. The authors start by explaining their reasons for using Python and R, and more importantly why both (avoiding the situation of having the hammer as your only tool). Next you learn how to set up Python using the &lt;a href=&#34;https://www.enthought.com/products/canopy/&#34;&gt;Canopy&lt;/a&gt; distribution and R using &lt;a href=&#34;http://www.rstudio.com/&#34;&gt;Rstudio&lt;/a&gt;. You also get some sample code to test your respective setups. Next the chapter introduces you to the concept of a data frame; a tabular data structure often used in data analysis. You gee a taste of both R&amp;rsquo;s native data.frame as well as Python&amp;rsquo;s DataFrame from the pandas package. lastly you&amp;rsquo;ll see how to organize your code for a typical analysis project. I recommend you don&amp;rsquo;t skip this chapter even if you&amp;rsquo;re familiar with either Python or R or even both.&lt;/p&gt;

&lt;h3 id=&#34;chapter-3-learning-the-hello-world-of-security-data-analysis&#34;&gt;Chapter 3: Learning the &amp;ldquo;Hello World&amp;rdquo; of Security Data Analysis&lt;/h3&gt;

&lt;p&gt;Chapter 3 is where things get real. You start by importing AlientVault&amp;rsquo;s IP Reputation database in your Python and R environment. You then get a feel of the data by performing some basic introspection of various fields and their data types and appropriate statistical summaries of them. Next you perform some basic charting using R&amp;rsquo;s &lt;a href=&#34;http://ggplot2.org/&#34;&gt;ggplot2&lt;/a&gt;and Python&amp;rsquo;s &lt;a href=&#34;http://www.matplotlib.org/&#34;&gt;Matplotlib&lt;/a&gt;. Even if you are familiar with basics of exploratory data analysis (EDA) I suggest you don&amp;rsquo;t skip the &amp;lsquo;Homing In on a Question&amp;rsquo; section, this is where you&amp;rsquo;ll learn how to use EDA for answering specifics questions about your data. There are quite a few examples here both in Python and R that use bar charts and &lt;a href=&#34;https://en.wikipedia.org/wiki/Heat_map&#34;&gt;heatmaps&lt;/a&gt; to explore relationships between various fields and derive answers from these relationships.&lt;/p&gt;

&lt;h3 id=&#34;chapter-4-performing-exploratory-security-data-analysis&#34;&gt;Chapter 4: Performing Exploratory Security Data Analysis&lt;/h3&gt;

&lt;p&gt;Chapter 4 dives into Exploratory Data Analysis (EDA) for InfoSec research. You get the know the the details about IPv4 addresses, and how they can be grouped together using Autonomous System Numbers (ASNs) and why that is useful. You also get to learn how to use a GeoIP service like Maxmind&amp;rsquo;s free &lt;a href=&#34;https://www.maxmind.com/en/geoip2-services-and-databases&#34;&gt;geoip&lt;/a&gt; API for tying an IP address to coordinates on a map. Once you have the geo coordinates you can use charting APIs to plot the IPs on a world map. After that you get to see how to augment IP addresses with other useful attributes such as the &lt;a href=&#34;https://en.wikipedia.org/wiki/Internet_Assigned_Numbers_Authority&#34;&gt;IANA&lt;/a&gt; block information for that IP. &lt;br/&gt;Next the chapter dives into basics of correlation analysis and lays down some core concepts behind correlation analysis. Lastly you get to build some graph data structures and visualize the graph nature of the relationships in IP addresses in the ZueS Botnet. All these techniques are foundations of initial exploratory analysis and although this chapter covers quite a bit of diverse but related concepts both from statistics as well as information security, it does a good job of tying all together. This will be the foundation for the analysis in later chapters.&lt;/p&gt;

&lt;h3 id=&#34;chapter-5-from-maps-to-regression&#34;&gt;Chapter 5: From Maps to Regression&lt;/h3&gt;

&lt;p&gt;Chapter 5 starts with basic concepts of plotting geographical maps, it walks you through plotting with latitude and longitude data, plotting per country stats using &lt;a href=&#34;https://en.wikipedia.org/wiki/Choropleth_map&#34;&gt;Choropleth&lt;/a&gt; plots, and zooming in on a specific country (USA in this example). Plotting some numbers on a geographical map is pointless unless it enables you to derive some information / insight from that plot. The chapter looks at a potentially interesting data point and then uses box plots to see if the data point is indeed an outlier. Finally the mapping part concludes by showing you how to aggregate the data at county level.&lt;br/&gt;The last part is a quick introduction to regression analysis (There are multitudes of books written on just this one subject). You get to learn how to build regression models and perform analysis based on model parameters. You also see some caveats you need to keep in mind when interpreting regression models. Finally you get to see how to apply regression analysis for seeing if reported alien sightings have any impact on the infection rate of ZeroAccess rootkit. Yes you read that right and the authors are not fools, they chose these 2 variables to prove a point about &lt;a href=&#34;https://en.wikipedia.org/wiki/Multicollinearity&#34;&gt;multicollinearity&lt;/a&gt; a common problem in regression analysis.&lt;/p&gt;

&lt;h3 id=&#34;chapter-6-visualizing-security-data&#34;&gt;Chapter 6: Visualizing Security Data&lt;/h3&gt;

&lt;p&gt;Chapter 6 is all about a picture speaking louder than a thousand words. Effective visualization is the key foundation of data analysis. The chapter starts with explaining the need for visualization and semi deep-dives into understanding visual perception and why it is important in building effective visualizations. These are topics that deserve their own books let alone chapters, but yet the authors manage to convey the gist of it all in the first few sections of the chapter.&lt;br/&gt;
The chapter then moves on to specific examples of visualization like bar charts, &lt;a href=&#34;https://en.wikipedia.org/wiki/Bubble_chart&#34;&gt;bubble charts&lt;/a&gt;, &lt;a href=&#34;https://en.wikipedia.org/wiki/Treemapping&#34;&gt;treemaps&lt;/a&gt;, distribution visualization using histogram and density plots. You also get a taste of visualizing time series data. Lastly you get to build a movie from your data. (I kid you not.)&lt;/p&gt;

&lt;h3 id=&#34;chapter-7-learning-from-security-breaches&#34;&gt;Chapter 7: Learning from Security Breaches&lt;/h3&gt;

&lt;p&gt;Chapter 7 devotes to the art of examining and analyzing security breaches. The authors introduce you to the &lt;a href=&#34;http://veriscommunity.net/&#34;&gt;Veris Framework&lt;/a&gt; developed by one of the authors for capturing information related to data breaches to be used in Verizon&amp;rsquo;s annual Data Breach Investigations Report (&lt;a href=&#34;http://www.verizonenterprise.com/DBIR/&#34;&gt;DBIR&lt;/a&gt;). Before examining the details of the VERIS f/w the authors explain why it is necessary to analyze data breaches, what sort of research questions can be answered and what are some of the considerations when designing a data collection framework for the same.&lt;br/&gt;Next the authors introduce the veris framework, its various sections, and enumerations used in them. You get to learn how VERIS tracks assets, actors, threats, actions, and how they affect Confidentiality, Integrity, &amp;amp; Availability (CIA triad) of the breached data. You also learn how to code up discovery/response and the subsequent impact of the data breach on the victim organization.&lt;br/&gt;Next you get to play with some real life database which is captured in the VERIS Community DataBase (&lt;a href=&#34;http://vcdb.org/&#34;&gt;VCDB&lt;/a&gt;). VCDB is a project used to capture publicly disclosed data breaches and encode them in the VERIS format. The VERIS format is a JSON specification, and you see code examples of doing basic uni-variate and bi-variate analysis like bar-charts and heatmaps.&lt;/p&gt;

&lt;h3 id=&#34;chapter-8-breaking-up-with-your-relational-database&#34;&gt;Chapter 8: Breaking Up with Your Relational Database&lt;/h3&gt;

&lt;p&gt;RDBMS , NOSQL and everything in between that&amp;rsquo;s what Chapter 9 is all about. With a quick primer on SQL/RDBMS you get to get your feet wet with &lt;a href=&#34;https://mariadb.org/&#34;&gt;MariaDB&lt;/a&gt; (MySQL fork), you learn how to create a small schema for storing InfoSec entities, as well as difference in terms of speed of a disk backed v/s memory backed storage engine. From RDBMS we move to NOSQL (Not Only SQL and not No SQL). The authors first explore &lt;a href=&#34;http://www.oracle.com/technetwork/database/database-technologies/berkeleydb/overview/index.html&#34;&gt;BerkeleyDB&lt;/a&gt; a very popular key-value datastore. You have sample code in both R and python for interaction with BerkeleyDB. Next the chapter deals with &lt;a href=&#34;http://redis.io&#34;&gt;Redis&lt;/a&gt; a very popular data-structure datastore. You learn about the various data structures supported by Redis and a couple of its advanced features. The authors also tackle Hadoop &amp;amp; MapReduce for processing security data at scale, and also touch base with &lt;a href=&#34;http://www.mongodb.org&#34;&gt;MongoDB&lt;/a&gt; and passing reference to &lt;a href=&#34;http://www.elastic.co&#34;&gt;elasticsearch&lt;/a&gt; and &lt;a href=&#34;http://www.neo4j.com&#34;&gt;Neo4J&lt;/a&gt;. Overall the chapter deals with some very popular RDBMSs and NOSQL databases, and provide you code samples to interact with them in python and R.&lt;/p&gt;

&lt;h3 id=&#34;chapter-9-demystifying-machine-learning&#34;&gt;Chapter 9:  Demystifying Machine Learning&lt;/h3&gt;

&lt;p&gt;Chapter 9 is all about Machine Learning in the InfoSec domain. Now let&amp;rsquo;s get this straight, ML is a very vast and widely spread topic. There are entire books devoted just to certain aspects of it. But even then the authors have managed to cover enough ground and should definitely pique your interest about ML if you haven&amp;rsquo;t been exposed to it yet. The chapter starts with defining ML, not an easy thing to do. The chapter shows you how to build a model to detect malware from non-malware using classification techniques. Then the chapter deals with model validation techniques/issues, risks of overfitting, feature selection which are some of the common things you do when building a ML model. Next the chapter looks at various supervised and unsupervised learning techniques. Finally you get 3 examples, clustering breach data, multidimensional scaling of victim industries, and hierarchical clustering of victim industries.&lt;br/&gt; It is impossible to do full justice to ML even in a whole book let alone a single chapter, but you still get enough to get you started.&lt;/p&gt;

&lt;h3 id=&#34;chapter-10-designing-effective-security-dashboards&#34;&gt;Chapter 10: Designing Effective Security Dashboards&lt;/h3&gt;

&lt;p&gt;A &amp;lsquo;Dashboard is not an Automobile&amp;rsquo;. Chapter 10 is about creating effective InfoSec Dashboards. The chapter introduces you to bullet graphs (a creation of &lt;a href=&#34;http://www.perceptualedge.com/about.php&#34;&gt;Stephen Few&lt;/a&gt;) as a much saner and efficient alternative to Gauges and dials. You also see examples of other interesting dashboard visualizations like Sparklines. The authors have some good advice about things to do and don&amp;rsquo;t when designing dashboards.&lt;br/&gt;Next the authors deal with a concrete example of conveying and managing security via Dashboards. The authors stress on the simple and yet extremely effective bar charts, and bullet graphs, as opposed to fancier but confusing UI elements like 3D charts, pie charts etc. To illustrate this point the authors have provided a couple of Dashboard makeover examples.&lt;br/&gt;Finally the authors talk about designing dashboards for InfoSec. Stressing on two simple questions a) What is going on ? &amp;amp; b) So what ?, the authors explain what should and what should not be presented on an InfoSec Dashboard and how most effectively to present it.&lt;/p&gt;

&lt;h3 id=&#34;chapter-11-building-interactive-security-visualizations&#34;&gt;Chapter 11: Building Interactive Security Visualizations&lt;/h3&gt;

&lt;p&gt;Chapter 11 is all about interactive visualizations, interactive being the keyword. You learn when to move from static to dynamic visualization, and more importantly why. As the authors point out prefer static and go dynamic only if dynamism &lt;strong&gt;augments&lt;/strong&gt; or aids in &lt;strong&gt;exploration&lt;/strong&gt; or &lt;strong&gt;illuminates&lt;/strong&gt; a topic in a way that can&amp;rsquo;t be done using static images. The authors present an example of each of these three cases and discuss the pros and cons of dynamic visualization in these context. Next the authors present ways to create dynamic visualizations using &lt;a href=&#34;https://www.tableau.com/&#34;&gt;Tableau&lt;/a&gt;, a very popular Business Intelligence and Visualization tool, and also using &lt;a href=&#34;http://d3js.org/&#34;&gt;D3.js&lt;/a&gt; a free and open source javascript charting library. As with other chapters you get to tie in this topic in InfoSec by designing an interactive threat explorer using jQuery, vega and opentip.&lt;/p&gt;

&lt;h3 id=&#34;chapter-12-moving-towards-data-driven-security&#34;&gt;Chapter 12: Moving Towards Data-Driven Security&lt;/h3&gt;

&lt;p&gt;The authors provide their own advice for InfoSec research based on their experience and acumen in Chapter 12. They recommend &amp;lsquo;panning for gold&amp;rsquo; rather than &amp;lsquo;drilling for oil&amp;rsquo;; that is to say not getting bogged down on a specific focus but explore the data and then focus on the questions you want to ask. They offer practical advice on various roles one can play in the InfoSec domain ranging from the Hacker, Coder, Data Munger, Visualizer, Thinker, Statistician to Security Domain Expert. For each role they provide a list of resources to sharpen your skill sets. Lastly they offer tips on moving your entire organization towards data-driven security and building security data teams.&lt;/p&gt;

&lt;h3 id=&#34;appendix&#34;&gt;Appendix&lt;/h3&gt;

&lt;p&gt;Appendix A provides a vast list of web links. From Data Cleansing, Analytics and Visualization tools, to aggregation sites and blogs to follow. There is a ton of material worth checking out and bookmarking here.&lt;/p&gt;

&lt;h2 id=&#34;conclusion-and-other-thoughts&#34;&gt;Conclusion and Other thoughts&lt;/h2&gt;

&lt;p&gt;So how do I rate this book ? This is a rather difficult question considering that nothing like this has ever been attempted before. Sure there are plenty of books about traditional InfoSec research and tools, and there are even more books on Statistics, and Machine Learning, and Visualization, not to mention gazillions of books on Programming/Coding. But a book that touches all 3 aspects of Data Science is indeed very rare.&lt;br/&gt;
Having said that I like this book very much, it covers every aspect of Data Science with a focus on InfoSec in just enough detail to give it justice. The code samples are great but more important is the very serious advice the authors have to offer (albeit in a lighter tone). This book is by no means a small achievement, not only in InfoSec books but Data Science books as well. I don&amp;rsquo;t see any reason why this books should not be in your collection if you deal with InfoSec and/or Data Science. Even if your domain is not InfoSec but if you are interested in Data Science I would still highly recommend this book as it will show you how to make Data Science work for your domain using InfoSec as an example.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to use Twitter’s Search REST API most effectively.</title>
      <link>/2015/01/how-to-use-twitters-search-rest-api-most-effectively./</link>
      <pubDate>Mon, 05 Jan 2015 12:49:00 +0000</pubDate>
      
      <guid>/2015/01/how-to-use-twitters-search-rest-api-most-effectively./</guid>
      <description>

&lt;p&gt;This blog post will discuss various techniques to use Twitter&amp;rsquo;s search REST API most effectively, given the constraints and limits of the said API. I&amp;rsquo;ll be using python for demonstration, but any native API which supports the Twitter REST API will do.&lt;/p&gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Twitter provides the &lt;a href=&#34;https://dev.twitter.com/rest/public/search&#34;&gt;REST search api&lt;/a&gt; for searching tweets from Twitter&amp;rsquo;s search index. This is different than using the &lt;a href=&#34;https://dev.twitter.com/streaming/reference/post/statuses/filter&#34;&gt;streaming filter API&lt;/a&gt;, in that the later is real-time and starts giving you results from the point of query, while the former is retrospective and will give you results from past, up to as far back as the search index goes (usually last 7 days).
While the streaming API seems like the thing to use when you want to track a certain query in real time, there are situations where you may want to use the regular REST search API. You may also want to combine the two approaches, i.e. start 2 searches, one using the streaming filter API to go forward in time and one using the REST search API to go backwards in time, in order to get some on-going and past context for your search term.&lt;/p&gt;

&lt;p&gt;Either way if the REST Search API is something you want to use, then there are a few limitations you need to be aware of and some techniques you can use to maximize the resources the API gives you. This post will explore approaches to use the REST search API optimally in order to find as much information as fast as possible and yet remain within the constraints of the API. To start with the &lt;a href=&#34;https://dev.twitter.com/rest/public/rate-limiting&#34;&gt;API Rate Limit&lt;/a&gt; page details the limits of various Twitter APIs, and as per the page the limit for the Search API is &lt;strong&gt;180 Requests per 15 mins window&lt;/strong&gt; for per-user authentication.
Now here&amp;rsquo;s the kicker, most code samples on the internet for the search API use the &lt;a href=&#34;https://dev.twitter.com/oauth/overview/application-owner-access-tokens&#34;&gt;Access Token Auth&lt;/a&gt; method, which is limited to the aforementioned 180 Requests/15 mins limit, and per request you can ask for maximum 100 tweets, giving you a grand total limit of &lt;strong&gt;18,000 tweets/15 mins&lt;/strong&gt;, If you download 18K tweets before 15 mins, you won&amp;rsquo;t be able to get any more results until your 15 min. window expires and you search again. Also you need to be aware of the following limitations of the search API.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Please note that Twitter’s search service and, by extension, the Search API is not meant to be an exhaustive source of Tweets. Not all Tweets will be indexed or made available via the search interface.&lt;/p&gt;
&lt;footer&gt;
&lt;cite&gt;&lt;a href=&#34;https://dev.twitter.com/rest/reference/get/search/tweets&#34;&gt;Reference for GET /search/tweets API Endpoint&lt;/a&gt;&lt;/cite&gt;
&lt;/footer&gt;
&lt;/blockquote&gt;

&lt;p&gt;and
&lt;blockquote&gt;
&lt;p&gt;Before getting involved, it’s important to know that the Search API is focused on relevance and not completeness. This means that some Tweets and users may be missing from search results. If you want to match for completeness you should consider using a Streaming API instead.&lt;/p&gt;
&lt;footer&gt;
&lt;cite&gt;&lt;a href=&#34;https://dev.twitter.com/rest/public/search&#34;&gt;The Search API&lt;/a&gt;&lt;/cite&gt;
&lt;/footer&gt;
&lt;/blockquote&gt;&lt;/p&gt;

&lt;p&gt;What this means is, using the search API you are not going to get all the tweets that match your search criteria, even if they are present in your desired timeframe. This is an important point to keep in mind when drawing conclusions about the size of the dataset obtained from using the search REST API.&lt;/p&gt;

&lt;h2 id=&#34;the-problem&#34;&gt;The problem&lt;/h2&gt;

&lt;p&gt;So given this background information, can we do something about the following points ?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Could we query at a rate faster than 18K tweets/15 mins ?&lt;/li&gt;
&lt;li&gt;Could we maintain a search context across our API rate limit window, so as to avoid getting duplicate results when searching repeatedly over a long period of time ?&lt;/li&gt;
&lt;li&gt;Could we do something about the fact that not all tweets matching the search criteria will be returned by the API ?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And the answer to all these 3 questions is YES. There wouldn&amp;rsquo;t be a point to this blog post if the answers were no, would there ?&lt;/p&gt;

&lt;h2 id=&#34;the-solution&#34;&gt;The Solution&lt;/h2&gt;

&lt;p&gt;I&amp;rsquo;ll be using python and the excellent &lt;a href=&#34;http://tweepy.readthedocs.org/en/v3.0.0/&#34;&gt;Tweepy&lt;/a&gt; API for this purpose, but any API in any programming language that supports Twitter&amp;rsquo;s REST APIs will do.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;T&lt;/strong&gt;o start with our first question about being able to search at a rate greater than 18K tweets/15 mins. The solution is to use &lt;a href=&#34;https://dev.twitter.com/oauth/application-only&#34;&gt;Application only Auth&lt;/a&gt; instead of the Access Token Auth. Application only auth has higher limits, precisely up to 450 request/sec and again with a limitation of requesting maximum 100 tweets per request, this gives a rate of &lt;strong&gt;45,000 tweets/15-min&lt;/strong&gt;, which is &lt;strong&gt;2.5 times&lt;/strong&gt; more than the Access Token Limit.&lt;/p&gt;

&lt;p&gt;The code sample below shows how to use App Only Auth using the Tweepy API.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tweepy

# Replace the API_KEY and API_SECRET with your application&#39;s key and secret.
auth = tweepy.AppAuthHandler(API_KEY, API_SECRET)
 
api = tweepy.API(auth, wait_on_rate_limit=True,
				   wait_on_rate_limit_notify=True)
 
if (not api):
    print (&amp;quot;Can&#39;t Authenticate&amp;quot;)
    sys.exit(-1)
 
# Continue with rest of code
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The secret is the &lt;code&gt;AppAuthHandler&lt;/code&gt; instead of the more frequent &lt;code&gt;OAuthHandler&lt;/code&gt; which you find being used in lots of code samples. This sets up App-only Auth and gives you higher limits.
Also as an added bonus notice the &lt;code&gt;wait_on_rate_limit&lt;/code&gt; &amp;amp; &lt;code&gt;wait_on_rate_limit_notify&lt;/code&gt; flags set to true. What this does is make the Tweepy API call auto wait (sleep) when it hits the rate limit and continue upon expiry of the window. This avoids you to have to program this part manually, which as you&amp;rsquo;ll shortly see makes your program much more simple and elegant.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;N&lt;/strong&gt;ext we tackle the second question about maintaining a search context when querying repeatedly over a long time frame. REST APIs by their very nature are stateless, i.e. there is no implicit context maintained by the server in between successive calls to the same API which can tell it what results have been sent to the client so far. So what we need is a way for the client to tell the API server where it is in a search result context, so that the server can then send the next set of results (This is called pagination). The search REST API allows this by accepting two input parameters as part of the API viz. &lt;code&gt;max_id&lt;/code&gt; &amp;amp; &lt;code&gt;since_id&lt;/code&gt; which serve as the upper and lower bounds of the unique IDs that Twitter assigns each tweet. By manipulating these two inputs during successive calls to the search API you can paginate your results.
Below is a code sample that does just that.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import sys
import jsonpickle
import os

searchQuery = &#39;#someHashtag&#39;  # this is what we&#39;re searching for
maxTweets = 10000000 # Some arbitrary large number
tweetsPerQry = 100  # this is the max the API permits
fName = &#39;tweets.txt&#39; # We&#39;ll store the tweets in a text file.


# If results from a specific ID onwards are reqd, set since_id to that ID.
# else default to no lower limit, go as far back as API allows
sinceId = None

# If results only below a specific ID are, set max_id to that ID.
# else default to no upper limit, start from the most recent tweet matching the search query.
max_id = -1L

tweetCount = 0
print(&amp;quot;Downloading max {0} tweets&amp;quot;.format(maxTweets))
with open(fName, &#39;w&#39;) as f:
    while tweetCount &amp;lt; maxTweets:
        try:
            if (max_id &amp;lt;= 0):
                if (not sinceId):
                    new_tweets = api.search(q=searchQuery, count=tweetsPerQry)
                else:
                    new_tweets = api.search(q=searchQuery, count=tweetsPerQry,
                                            since_id=sinceId)
            else:
                if (not sinceId):
                    new_tweets = api.search(q=searchQuery, count=tweetsPerQry,
                                            max_id=str(max_id - 1))
                else:
                    new_tweets = api.search(q=searchQuery, count=tweetsPerQry,
                                            max_id=str(max_id - 1),
                                            since_id=sinceId)
            if not new_tweets:
                print(&amp;quot;No more tweets found&amp;quot;)
                break
            for tweet in new_tweets:
                f.write(jsonpickle.encode(tweet._json, unpicklable=False) +
                        &#39;\n&#39;)
            tweetCount += len(new_tweets)
            print(&amp;quot;Downloaded {0} tweets&amp;quot;.format(tweetCount))
            max_id = new_tweets[-1].id
        except tweepy.TweepError as e:
            # Just exit if any error
            print(&amp;quot;some error : &amp;quot; + str(e))
            break

print (&amp;quot;Downloaded {0} tweets, Saved to {1}&amp;quot;.format(tweetCount, fName))

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The above code will write all the downloaded tweets in a text file. Each line representing a tweet encoded in JSON format. The tweets in the file are in reversed order of the creation timestamp i.e. going from most recent to most farthest. There&amp;rsquo;s probably some room for beautifying the above code, but it works and can download literally millions of tweets at the optimal rate of 45K tweets/15-mins. Just run the code in a background process and it will go back as far as the search API allows until it has exhausted all the results. What&amp;rsquo;s more using the initial values for &lt;code&gt;max_id&lt;/code&gt; and/or &lt;code&gt;since_id&lt;/code&gt; you can fetch results to and from arbitrary IDs. This is really helpful if you want to the program repeatedly to fetch newer results since last run. Just look up the max ID (the ID of the first line) from the previous run and set that to &lt;code&gt;since_id&lt;/code&gt; for the next run. If you&amp;rsquo;ve to stop your program before exhausting all the possible results and rerun it again to fetch the remaining results, you can look up the min ID (the ID of the last line) and pass that as &lt;code&gt;max_id&lt;/code&gt; for the next run to start from that ID and below.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;N&lt;/strong&gt;ow we look at our third question, given the fact that the search results will not contain all possible matching tweets, can we do something about it ? The answer is yes, but it gets a bit tricky. The idea is that; Of the tweets you have fetched there will be quite a lot of retweets, and chances are that some of the original tweets of these retweets are not in the results downloaded. But each retweet also encodes the entire original tweet object in its JSON representation. So if we pick out these original tweets from retweets then we can augment our results by including the missing original tweets in the result set. We can easily do this as each tweet is assigned a unique ID, thus allowing us to use set functions to pick out only the missing tweets.&lt;/p&gt;

&lt;p&gt;This approach is not as complicated as it sounds, and can be easily accomplished in any programming language. I have a working code written in R (not shown here). I leave it as an exercise to the reader to implement it in python or whichever language of his/her choice. From my tests for various search queries , I get anywhere from 2% to 10% more tweets this way, so it&amp;rsquo;s a worthwhile exercise, and it completes your dataset in that you have all the original tweets of every retweet found in your dataset.&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;I highlighted some of the limitations of Twitter&amp;rsquo;s search REST API; how you can best use it to the fullest allowed rate limit. I also explained approaches to paginate results as well as extending the result set by another 2% to 10% by extracting missing original tweets from the retweets. Using these approaches you should be able to download a whole lot more tweets at a much faster rate.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Technical Notes&lt;/em&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Tweepy also has a &lt;code&gt;api.Cursor&lt;/code&gt; &lt;a href=&#34;http://docs.tweepy.org/en/latest/cursor_tutorial.html&#34;&gt;method &lt;/a&gt; which could possibly replace the whole while loop in the second code sample, but it seems the Cursor API suffers from memory leak and will eventually &lt;a href=&#34;https://stackoverflow.com/questions/22469713/managing-tweepy-api-search/23996991#comment37338657_22473254&#34;&gt;crash your program&lt;/a&gt;. Hence my approach is based on modification of &lt;a href=&#34;http://stackoverflow.com/a/22473254&#34;&gt;this&lt;/a&gt; answer on stackoverflow.&lt;/li&gt;
&lt;li&gt;For extracting the missing original tweets from retweets, think of the following pseudo-code.

&lt;ul&gt;
&lt;li&gt;Store all downloaded tweets in a set (say set A)&lt;/li&gt;
&lt;li&gt;From this set filter out the retweets &amp;amp;
extract the original tweet from these retweets (say set B)&lt;/li&gt;
&lt;li&gt;Insert in set A all unique tweets from set B that are not already in set A&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>