<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Api on Bhaskar Karambelkar&#39;s Blog</title>
    <link>/tags/api/index.xml</link>
    <description>Recent content in Api on Bhaskar Karambelkar&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; Bhaskar V. Karambelkar</copyright>
    <atom:link href="/tags/api/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>How to use Twitter’s Search REST API most effectively.</title>
      <link>/2015/01/how-to-use-twitters-search-rest-api-most-effectively./</link>
      <pubDate>Mon, 05 Jan 2015 12:49:00 +0000</pubDate>
      
      <guid>/2015/01/how-to-use-twitters-search-rest-api-most-effectively./</guid>
      <description>

&lt;p&gt;This blog post will discuss various techniques to use Twitter&amp;rsquo;s search REST API most effectively, given the constraints and limits of the said API. I&amp;rsquo;ll be using python for demonstration, but any native API which supports the Twitter REST API will do.&lt;/p&gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Twitter provides the &lt;a href=&#34;https://dev.twitter.com/rest/public/search&#34;&gt;REST search api&lt;/a&gt; for searching tweets from Twitter&amp;rsquo;s search index. This is different than using the &lt;a href=&#34;https://dev.twitter.com/streaming/reference/post/statuses/filter&#34;&gt;streaming filter API&lt;/a&gt;, in that the later is real-time and starts giving you results from the point of query, while the former is retrospective and will give you results from past, up to as far back as the search index goes (usually last 7 days).
While the streaming API seems like the thing to use when you want to track a certain query in real time, there are situations where you may want to use the regular REST search API. You may also want to combine the two approaches, i.e. start 2 searches, one using the streaming filter API to go forward in time and one using the REST search API to go backwards in time, in order to get some on-going and past context for your search term.&lt;/p&gt;

&lt;p&gt;Either way if the REST Search API is something you want to use, then there are a few limitations you need to be aware of and some techniques you can use to maximize the resources the API gives you. This post will explore approaches to use the REST search API optimally in order to find as much information as fast as possible and yet remain within the constraints of the API. To start with the &lt;a href=&#34;https://dev.twitter.com/rest/public/rate-limiting&#34;&gt;API Rate Limit&lt;/a&gt; page details the limits of various Twitter APIs, and as per the page the limit for the Search API is &lt;strong&gt;180 Requests per 15 mins window&lt;/strong&gt; for per-user authentication.
Now here&amp;rsquo;s the kicker, most code samples on the internet for the search API use the &lt;a href=&#34;https://dev.twitter.com/oauth/overview/application-owner-access-tokens&#34;&gt;Access Token Auth&lt;/a&gt; method, which is limited to the aforementioned 180 Requests/15 mins limit, and per request you can ask for maximum 100 tweets, giving you a grand total limit of &lt;strong&gt;18,000 tweets/15 mins&lt;/strong&gt;, If you download 18K tweets before 15 mins, you won&amp;rsquo;t be able to get any more results until your 15 min. window expires and you search again. Also you need to be aware of the following limitations of the search API.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Please note that Twitter’s search service and, by extension, the Search API is not meant to be an exhaustive source of Tweets. Not all Tweets will be indexed or made available via the search interface.&lt;/p&gt;
&lt;footer&gt;
&lt;cite&gt;&lt;a href=&#34;https://dev.twitter.com/rest/reference/get/search/tweets&#34;&gt;Reference for GET /search/tweets API Endpoint&lt;/a&gt;&lt;/cite&gt;
&lt;/footer&gt;
&lt;/blockquote&gt;

&lt;p&gt;and
&lt;blockquote&gt;
&lt;p&gt;Before getting involved, it’s important to know that the Search API is focused on relevance and not completeness. This means that some Tweets and users may be missing from search results. If you want to match for completeness you should consider using a Streaming API instead.&lt;/p&gt;
&lt;footer&gt;
&lt;cite&gt;&lt;a href=&#34;https://dev.twitter.com/rest/public/search&#34;&gt;The Search API&lt;/a&gt;&lt;/cite&gt;
&lt;/footer&gt;
&lt;/blockquote&gt;&lt;/p&gt;

&lt;p&gt;What this means is, using the search API you are not going to get all the tweets that match your search criteria, even if they are present in your desired timeframe. This is an important point to keep in mind when drawing conclusions about the size of the dataset obtained from using the search REST API.&lt;/p&gt;

&lt;h2 id=&#34;the-problem&#34;&gt;The problem&lt;/h2&gt;

&lt;p&gt;So given this background information, can we do something about the following points ?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Could we query at a rate faster than 18K tweets/15 mins ?&lt;/li&gt;
&lt;li&gt;Could we maintain a search context across our API rate limit window, so as to avoid getting duplicate results when searching repeatedly over a long period of time ?&lt;/li&gt;
&lt;li&gt;Could we do something about the fact that not all tweets matching the search criteria will be returned by the API ?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And the answer to all these 3 questions is YES. There wouldn&amp;rsquo;t be a point to this blog post if the answers were no, would there ?&lt;/p&gt;

&lt;h2 id=&#34;the-solution&#34;&gt;The Solution&lt;/h2&gt;

&lt;p&gt;I&amp;rsquo;ll be using python and the excellent &lt;a href=&#34;http://tweepy.readthedocs.org/en/v3.0.0/&#34;&gt;Tweepy&lt;/a&gt; API for this purpose, but any API in any programming language that supports Twitter&amp;rsquo;s REST APIs will do.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;T&lt;/strong&gt;o start with our first question about being able to search at a rate greater than 18K tweets/15 mins. The solution is to use &lt;a href=&#34;https://dev.twitter.com/oauth/application-only&#34;&gt;Application only Auth&lt;/a&gt; instead of the Access Token Auth. Application only auth has higher limits, precisely up to 450 request/sec and again with a limitation of requesting maximum 100 tweets per request, this gives a rate of &lt;strong&gt;45,000 tweets/15-min&lt;/strong&gt;, which is &lt;strong&gt;2.5 times&lt;/strong&gt; more than the Access Token Limit.&lt;/p&gt;

&lt;p&gt;The code sample below shows how to use App Only Auth using the Tweepy API.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tweepy

# Replace the API_KEY and API_SECRET with your application&#39;s key and secret.
auth = tweepy.AppAuthHandler(API_KEY, API_SECRET)
 
api = tweepy.API(auth, wait_on_rate_limit=True,
				   wait_on_rate_limit_notify=True)
 
if (not api):
    print (&amp;quot;Can&#39;t Authenticate&amp;quot;)
    sys.exit(-1)
 
# Continue with rest of code
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The secret is the &lt;code&gt;AppAuthHandler&lt;/code&gt; instead of the more frequent &lt;code&gt;OAuthHandler&lt;/code&gt; which you find being used in lots of code samples. This sets up App-only Auth and gives you higher limits.
Also as an added bonus notice the &lt;code&gt;wait_on_rate_limit&lt;/code&gt; &amp;amp; &lt;code&gt;wait_on_rate_limit_notify&lt;/code&gt; flags set to true. What this does is make the Tweepy API call auto wait (sleep) when it hits the rate limit and continue upon expiry of the window. This avoids you to have to program this part manually, which as you&amp;rsquo;ll shortly see makes your program much more simple and elegant.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;N&lt;/strong&gt;ext we tackle the second question about maintaining a search context when querying repeatedly over a long time frame. REST APIs by their very nature are stateless, i.e. there is no implicit context maintained by the server in between successive calls to the same API which can tell it what results have been sent to the client so far. So what we need is a way for the client to tell the API server where it is in a search result context, so that the server can then send the next set of results (This is called pagination). The search REST API allows this by accepting two input parameters as part of the API viz. &lt;code&gt;max_id&lt;/code&gt; &amp;amp; &lt;code&gt;since_id&lt;/code&gt; which serve as the upper and lower bounds of the unique IDs that Twitter assigns each tweet. By manipulating these two inputs during successive calls to the search API you can paginate your results.
Below is a code sample that does just that.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import sys
import jsonpickle
import os

searchQuery = &#39;#someHashtag&#39;  # this is what we&#39;re searching for
maxTweets = 10000000 # Some arbitrary large number
tweetsPerQry = 100  # this is the max the API permits
fName = &#39;tweets.txt&#39; # We&#39;ll store the tweets in a text file.


# If results from a specific ID onwards are reqd, set since_id to that ID.
# else default to no lower limit, go as far back as API allows
sinceId = None

# If results only below a specific ID are, set max_id to that ID.
# else default to no upper limit, start from the most recent tweet matching the search query.
max_id = -1L

tweetCount = 0
print(&amp;quot;Downloading max {0} tweets&amp;quot;.format(maxTweets))
with open(fName, &#39;w&#39;) as f:
    while tweetCount &amp;lt; maxTweets:
        try:
            if (max_id &amp;lt;= 0):
                if (not sinceId):
                    new_tweets = api.search(q=searchQuery, count=tweetsPerQry)
                else:
                    new_tweets = api.search(q=searchQuery, count=tweetsPerQry,
                                            since_id=sinceId)
            else:
                if (not sinceId):
                    new_tweets = api.search(q=searchQuery, count=tweetsPerQry,
                                            max_id=str(max_id - 1))
                else:
                    new_tweets = api.search(q=searchQuery, count=tweetsPerQry,
                                            max_id=str(max_id - 1),
                                            since_id=sinceId)
            if not new_tweets:
                print(&amp;quot;No more tweets found&amp;quot;)
                break
            for tweet in new_tweets:
                f.write(jsonpickle.encode(tweet._json, unpicklable=False) +
                        &#39;\n&#39;)
            tweetCount += len(new_tweets)
            print(&amp;quot;Downloaded {0} tweets&amp;quot;.format(tweetCount))
            max_id = new_tweets[-1].id
        except tweepy.TweepError as e:
            # Just exit if any error
            print(&amp;quot;some error : &amp;quot; + str(e))
            break

print (&amp;quot;Downloaded {0} tweets, Saved to {1}&amp;quot;.format(tweetCount, fName))

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The above code will write all the downloaded tweets in a text file. Each line representing a tweet encoded in JSON format. The tweets in the file are in reversed order of the creation timestamp i.e. going from most recent to most farthest. There&amp;rsquo;s probably some room for beautifying the above code, but it works and can download literally millions of tweets at the optimal rate of 45K tweets/15-mins. Just run the code in a background process and it will go back as far as the search API allows until it has exhausted all the results. What&amp;rsquo;s more using the initial values for &lt;code&gt;max_id&lt;/code&gt; and/or &lt;code&gt;since_id&lt;/code&gt; you can fetch results to and from arbitrary IDs. This is really helpful if you want to the program repeatedly to fetch newer results since last run. Just look up the max ID (the ID of the first line) from the previous run and set that to &lt;code&gt;since_id&lt;/code&gt; for the next run. If you&amp;rsquo;ve to stop your program before exhausting all the possible results and rerun it again to fetch the remaining results, you can look up the min ID (the ID of the last line) and pass that as &lt;code&gt;max_id&lt;/code&gt; for the next run to start from that ID and below.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;N&lt;/strong&gt;ow we look at our third question, given the fact that the search results will not contain all possible matching tweets, can we do something about it ? The answer is yes, but it gets a bit tricky. The idea is that; Of the tweets you have fetched there will be quite a lot of retweets, and chances are that some of the original tweets of these retweets are not in the results downloaded. But each retweet also encodes the entire original tweet object in its JSON representation. So if we pick out these original tweets from retweets then we can augment our results by including the missing original tweets in the result set. We can easily do this as each tweet is assigned a unique ID, thus allowing us to use set functions to pick out only the missing tweets.&lt;/p&gt;

&lt;p&gt;This approach is not as complicated as it sounds, and can be easily accomplished in any programming language. I have a working code written in R (not shown here). I leave it as an exercise to the reader to implement it in python or whichever language of his/her choice. From my tests for various search queries , I get anywhere from 2% to 10% more tweets this way, so it&amp;rsquo;s a worthwhile exercise, and it completes your dataset in that you have all the original tweets of every retweet found in your dataset.&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;I highlighted some of the limitations of Twitter&amp;rsquo;s search REST API; how you can best use it to the fullest allowed rate limit. I also explained approaches to paginate results as well as extending the result set by another 2% to 10% by extracting missing original tweets from the retweets. Using these approaches you should be able to download a whole lot more tweets at a much faster rate.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Technical Notes&lt;/em&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Tweepy also has a &lt;code&gt;api.Cursor&lt;/code&gt; &lt;a href=&#34;http://docs.tweepy.org/en/latest/cursor_tutorial.html&#34;&gt;method &lt;/a&gt; which could possibly replace the whole while loop in the second code sample, but it seems the Cursor API suffers from memory leak and will eventually &lt;a href=&#34;https://stackoverflow.com/questions/22469713/managing-tweepy-api-search/23996991#comment37338657_22473254&#34;&gt;crash your program&lt;/a&gt;. Hence my approach is based on modification of &lt;a href=&#34;http://stackoverflow.com/a/22473254&#34;&gt;this&lt;/a&gt; answer on stackoverflow.&lt;/li&gt;
&lt;li&gt;For extracting the missing original tweets from retweets, think of the following pseudo-code.

&lt;ul&gt;
&lt;li&gt;Store all downloaded tweets in a set (say set A)&lt;/li&gt;
&lt;li&gt;From this set filter out the retweets &amp;amp;
extract the original tweet from these retweets (say set B)&lt;/li&gt;
&lt;li&gt;Insert in set A all unique tweets from set B that are not already in set A&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Searching, Analyzing &amp; Visualizing Security Feeds</title>
      <link>/2014/02/searching-analyzing--visualizing-security-feeds/</link>
      <pubDate>Sat, 22 Feb 2014 16:27:00 +0000</pubDate>
      
      <guid>/2014/02/searching-analyzing--visualizing-security-feeds/</guid>
      <description>&lt;p&gt;&lt;strong&gt;I&lt;/strong&gt;f you work with Computer or Network Security, then terms like &lt;a href=&#34;https://cve.mitre.org/&#34;&gt;CVE&lt;/a&gt;, &lt;a href=&#34;https://cpe.mitre.org/&#34;&gt;CPE&lt;/a&gt;, &lt;a href=&#34;https://cwe.mitre.org/&#34;&gt;CWE&lt;/a&gt;, &lt;a href=&#34;http://nvd.nist.gov/cce/index.cfm&#34;&gt;CCE&lt;/a&gt;, etc. should be very familiar to you. If not, you&amp;rsquo;re in the wrong field :).&lt;/p&gt;

&lt;p&gt;For those who don&amp;rsquo;t work in these fields but are curious about it, these are some of the security related feeds provided by independent organizations such as &lt;a href=&#34;http://www.mitre.org/&#34;&gt;MITRE&lt;/a&gt; or &lt;a href=&#34;http://www.nist.gov/&#34;&gt;NIST&lt;/a&gt; and are part of the &amp;ldquo;&lt;a href=&#34;https://measurablesecurity.mitre.org/&#34;&gt;Making Security Measurable&lt;/a&gt;&amp;rdquo; initiative. These feeds provide meta data about things related to Computer/Network Security such as standard names for platforms/operating-systems/software/hardware, standard names for common vulnerabilities, weakness, configurations. Using these standard names help different vendors identify and tag security vulnerabilities, platforms, etc in a non-ambiguous way.
Almost any vendor in this space relies on these feeds, and incorporates them in their products in some way or another. We use them too, but &amp;hellip;..&lt;/p&gt;

&lt;p&gt;We have more than one security related products, and each provides a unique take on Computer/Network security, and this is not unique to our way of working. You&amp;rsquo;ll see this pattern in the whole of security industry, organizations having products catering to SIEM (Security Information and Event Management), VM (Vulnerability Management), Log Management, Compliance etc.&lt;/p&gt;

&lt;p&gt;In our case each of these product offerings intake some or all of these feeds (which are provided in XML format), parse the feed and load it in a RDBMS. 
The problems with this approach are &amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Each product team has it&amp;rsquo;s own code base for parsing these feeds and it&amp;rsquo;s own DB schema for representing these feeds in DB. So a lot of work is duplicated, and there are no standards across products on how to model these feeds in each product.&lt;/li&gt;
&lt;li&gt;Each Product only takes in a subset of the available feeds, and brings in new feeds if and when needed. So it&amp;rsquo;s a never ending cycle for each new feed. Write parsers, design DB schma, and code the ETL procedures.&lt;/li&gt;
&lt;li&gt;Due the rather complex nature of these feed formats, and rather limited ability to model such complex structures in a RDBMS, we end up throwing away a lot of information and cherry pick only important attributes such as ID, name, description, title etc. and keep our models simple. &lt;/li&gt;
&lt;li&gt;And perhaps the most important, no text search capability within or across feeds. These feeds have some very elaborate descriptions , code samples, so full  text search is very critical.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So while this approach works , as you can see it&amp;rsquo;s not very efficient, duplication of work, no standard model across products and limitations of the storage platform result in discarding information that could potentially be useful.&lt;/p&gt;

&lt;p&gt;So what&amp;rsquo;s needed is&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A independent meta store that can handle any and all available feeds.&lt;/li&gt;
&lt;li&gt;A flexible storage platform not limited by shortcomings of a RDBMS system and in turn can retain almost all available information in the feeds.&lt;/li&gt;
&lt;li&gt;A very simple REST API for each product to tap in to the meta store.&lt;/li&gt;
&lt;li&gt;A full text search as well as a field based search interface as part of the REST API.&lt;/li&gt;
&lt;li&gt;A framework / API for descriptive statistical analysis, exploratory analysis on the information store.&lt;/li&gt;
&lt;li&gt;A very simple dashboard to visualize these feeds, for birds eye view.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As most of these feeds are published as XMLs , initially I thought of using a &amp;lsquo;XML DB&amp;rsquo; for the job. I have quite a bit of experience with &amp;lsquo;Oracle XMLDB&amp;rsquo;, but although it can offer a much flexible platform for storage, we&amp;rsquo;ll still need to define a proper DB schema based on the underlying XML schema, to take full advantage of &amp;lsquo;XMLDB&amp;rsquo; features, and we&amp;rsquo;ll still need to write our one API layer on top of XMLDB. So the amount of efforts is not reduced significantly. Not to mention any statistical analysis or dashboarding is added effort.&lt;/p&gt;

&lt;p&gt;So what&amp;rsquo;s the alternative for developing such a solution ? &amp;hellip;. &lt;strong&gt;&lt;a href=&#34;http://www.elasticsearch.org/&#34;&gt;ElasticSearch&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Why ? , because &amp;hellip;..&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ES offers a very flexible storage model, not only is it a full text search product but a very competent storage platform for unstructured or semi-structured data (NoSQL DB if you will).&lt;/li&gt;
&lt;li&gt;It works very well schema less, and efforts to define a Index Mapping (equivalent to a DB Schema) are very minimal compared to a traditional RDBMS schema designing. So you can work in a mix environment where you define mappings for a key set of attributes and leave the rest for ES&amp;rsquo;s dynamic mapping.&lt;/li&gt;
&lt;li&gt;Full text search is ES&amp;rsquo;s bread and butter, and it also works well for field based querying/filtering.&lt;/li&gt;
&lt;li&gt;ES provides statistical APIs (facets in pre-1.0 release, and aggregations in 1.0+ releases) out of box, without us having to write a single piece of code.&lt;/li&gt;
&lt;li&gt;ES provides &lt;a href=&#34;http://www.elasticsearch.org/overview/kibana/&#34;&gt;kibana&lt;/a&gt; for building dashboards on data stored in ES. Kibana does all the heavy lifting and you can build very intuitive dashboards in a very short amount of time.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So what&amp;rsquo;s need to be done by us ? Well not much really, here&amp;rsquo;s what I&amp;rsquo;ve done so far..&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A simple perl script which can download data feeds (XMLs), convert XML to JSON (trust me with perl this is a breeze, 2 lines of code ), minimal normalization of data if needed and then use elasticsearch perl API to bulk index the feed. (The whole code is about 100 to 120 lines).&lt;/li&gt;
&lt;li&gt;Define a minimal set of mappings for the feeds. Again the idea is to make heavy use of ES&amp;rsquo; dynamic mappings for most fields and only provide explicit mappings for a select few key attributes.&lt;/li&gt;
&lt;li&gt;Built kibana dashboards for search as well as summarizing feed data in graphs.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In short the coding effort was a very small perl script, es mapping template, and kibana dashboard building, all accomplished in a matter of hours, as opposed to the current approach which requires days/weeks for each new feed we want to work with.&lt;/p&gt;

&lt;p&gt;Overall I&amp;rsquo;m very pleased and satisfied with what has been achieved. Below are some Kibana dashboards I&amp;rsquo;ve build.
Please note that as nice as Kibana is, what&amp;rsquo;s more important is the full text search capabilities that we get from ES, and a very easy and intuitive REST API, which can be used by any product to tap in to this feed store, that&amp;rsquo;s more important to us. Not to mention the ridiculously small amount of time spent to put this all together. &lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 1: CVEs by Score (Also Adobe Ouch..)&lt;/em&gt;
&lt;a href=&#34;https://lh3.googleusercontent.com/-zmW5utCjOQU/UwkRPZucr3I/AAAAAAAABcQ/lp82qVehue0/s1440/CVEs_by_score.png&#34; data-lightbox=&#34;MITRE&#34; data-title=&#34; CVEs by Score (Also Adobe Ouch..)&#34; &gt;
    &lt;img src=&#34;http://2.bp.blogspot.com/-zmW5utCjOQU/UwkRPZucr3I/AAAAAAAABcQ/lp82qVehue0/s1600/CVEs_by_score.png&#34; /&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 2: CVEs by OS&lt;/em&gt;
&lt;a href=&#34;http://2.bp.blogspot.com/-WZdr1Sycvy0/UwkROoI2WRI/AAAAAAAABb0/PcrouIMtn-w/s1600/CVEs.png&#34; data-lightbox=&#34;MITRE&#34; data-title=&#34; CVEs by OS&#34; &gt;
    &lt;img src=&#34;http://2.bp.blogspot.com/-WZdr1Sycvy0/UwkROoI2WRI/AAAAAAAABb0/PcrouIMtn-w/s1600/CVEs.png&#34; /&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 3: CPEs&lt;/em&gt;
&lt;a href=&#34;http://4.bp.blogspot.com/-MPTLcLabgZs/UwkROm7ZdiI/AAAAAAAABbw/VSUd2ABfkyI/s1600/CPEs.png&#34; data-lightbox=&#34;MITRE&#34; data-title=&#34; CPEs&#34; &gt;
    &lt;img src=&#34;http://4.bp.blogspot.com/-MPTLcLabgZs/UwkROm7ZdiI/AAAAAAAABbw/VSUd2ABfkyI/s1600/CPEs.png&#34; /&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 4: CCEs&lt;/em&gt;
&lt;a href=&#34;http://2.bp.blogspot.com/-Mj4gMKx6ew4/UwkROi1hMkI/AAAAAAAABb8/bP-OfXcRx2Y/s1600/CCEs.png&#34; data-lightbox=&#34;MITRE&#34; data-title=&#34; CCEs&#34; &gt;
    &lt;img src=&#34;http://2.bp.blogspot.com/-Mj4gMKx6ew4/UwkROi1hMkI/AAAAAAAABb8/bP-OfXcRx2Y/s1600/CCEs.png&#34; /&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 5: CWEs&lt;/em&gt;
&lt;a href=&#34;http://2.bp.blogspot.com/-WqK791vz-us/UwkRPgpTD8I/AAAAAAAABcI/8DJgmphabvk/s1600/CWE.png&#34; data-lightbox=&#34;MITRE&#34; data-title=&#34; CWEs&#34; &gt;
    &lt;img src=&#34;http://2.bp.blogspot.com/-WqK791vz-us/UwkRPgpTD8I/AAAAAAAABcI/8DJgmphabvk/s1600/CWE.png&#34; /&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 6: CCEs&lt;/em&gt;
&lt;a href=&#34;http://3.bp.blogspot.com/-uzvuBrP5V5o/UwkRPsd2TtI/AAAAAAAABcM/4DwqaDjRSzY/s1600/NVDCCEs.png&#34; data-lightbox=&#34;MITRE&#34; data-title=&#34; CCEs&#34; &gt;
    &lt;img src=&#34;http://3.bp.blogspot.com/-uzvuBrP5V5o/UwkRPsd2TtI/AAAAAAAABcM/4DwqaDjRSzY/s1600/NVDCCEs.png&#34; /&gt;
&lt;/a&gt;
&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>